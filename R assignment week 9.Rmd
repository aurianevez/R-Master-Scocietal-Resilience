---
title: "R assignment week 9"
output:
  html_document:
    df_print: paged
---

Deadline for this week: 29 November 23:59. 

Before your do this assignment, please read Wickham chapter 10 (tibbles), complete the GitHub module "Text Analysis", and watch the instruction video for week 9 (see Canvas).

## 0) Seting working directory 

```{r}
setwd("~/Master/BigdataSmallData")
```


## 1) Load the tidyverse AND quanteda packages and import the COVID-19 sample dataset
Write down your code in the grey box.
```{r, message = FALSE}
library(tidyverse)
library(quanteda)
```

```{r, message = FALSE}
data <- read_csv("BDSD_tweets.csv")

```

## 2) Replicate an earlier step
Create the popular column. Write down your code in the grey box.

```{r}

# creating the popular variable
data <- mutate(data, popular = ifelse(user_followers_count > 1000, 1, 0))

```

## 3) Create a corpus of the tweets
Construct a corpus object from the COVID-19 tweets. Use the text column as input. Write down your code in the grey box.

```{r}
# checking text is character
# sapply(data, class)

mycorpus <- data %>% 
  corpus(text_field = "text")

```

```{r}
#checking
# head(mycorpus)
```


## 4) Create a dtm of the tweets
Create a document-term matrix (dtm) using the corpus you have created. Convert the text to lowercase and use stemming to make sure that the same words are grouped together. Also remove stopwords and punctuation to reduce the size of the dtm. Write down your code in the grey box.

```{r}
#stem --> stemming " stemming removes some parts at the ends of words to ignore different forms of the same word, such as singular versus plural and different verb forms"

dtm <- dfm(mycorpus, tolower=T, remove = stopwords('en'), stem = T, remove_punct=T)
  

```

```{r}
# checking
head(dtm)
```


## 5) Create a wordcloud
Create a wordcloud with a dark green font of the 100 most frequent words. Write down your code in the grey box.

```{r}

textplot_wordcloud(dtm, max_words = 100, color = c('dark green'))

```

## 6) Clean up dtm more
As you can see, one of the most frequent "words" are symbols like the euro sign and letters with accents not associated with the English language such as the grave (à) and circumflex (â). You could also argue that very short words like "I" and "go" have less informative value than many of the longer words. Therefore, use the function dtm_select and the regular expression skills you have obtained last week to remove strings that are shorter than three characters. Write down your code in the grey box.

```{r}


 dtm <- dfm_select(dtm, min_nchar = 4)

```

## 7) Create a second wordcloud
Create a wordcloud again to see if the words displayed are indeed more informative n
textplot_wordcloud(dtm, max_words = 100, color = c('dark green'))

```{r}
textplot_wordcloud(dtm, max_words = 100, color = c('dark green'))
```


## 8) Compare compora
Compare word frequencies between tweets published by popular vs. "unpopular" accounts. Use the second dtm you have created, the one that does not include symbols, accented single letters and very short words. Print these results using the textplot_keyness function. Write down your code in the grey box.

```{r}

is_popular <- docvars(dtm)$popular == 1
ts <- textstat_keyness(dtm, is_popular)

head(ts, 20)

textplot_keyness(ts, n = 10)


```

## 9a) Import the moral foundations dictionary
We will now practice with performing dictionary searches using an existing dictionary. According to moral foundations theory (Haidt & Graham, 2007), individuals make different moral judgments because they rely differently on five distinct moral values. The moral foundations dictionary aims to measure attention to these moral values. Download the dictionary from Canvas (.dic file) and use the dictionary function to import it into R Studio. Write down your code in the grey box.

```{r}

dico <- dictionary(file = "dico.dic", format = "LIWC")

```

## 9b) Apply the dictionary
Now, (1) perform a dictionary search using the moral foundations dictionary, (2) convert the results to a dataframe, and (3) print the column sums of each dictionary category (e.g., the number of times words related to care.virtue were found in the dataset). Write down your code in the grey box.  

```{r}

# dictionary search 
search = dfm_lookup(dtm, dico, exclusive = TRUE)

# convert results to dataframe
dtm_2 <- convert(search, to = "data.frame")

sum_category <- colSums(dtm_2[,-1])



```

## 9c) Conduct a MANOVA
Conduct a MANOVA to test whether tweets published by popular vs. unpopular accounts significantly contained different numbers of words related to only the care and fairness values (four variables). Write down your code in the grey box. Also print the output of the statistical test.

```{r}

# not for myself - I want to know something about popular and unpopular variable therefore it is my outcome variable - defendant variable. Care ans fairness values are the independent variable. 

# Hannah's code

data <- data %>% 
  mutate(ID = row_number())

dtm_2 <- dtm_2 %>%
  mutate (ID = row_number ())

total <- merge (data, dtm_2, by = "ID")

manova <- manova(cbind(care.virtue,care.vice,fairness.virtue,fairness.vice) ~ popular, data = total)

summary(manova)
summary.aov(manova)


```

## 9d) Retrieve Ms and SDs
Apparently, there are only significant differences between popular vs. unpopular accounts in terms of attention to care. Print the means and standard deviations of both groups of accounts to see which group paid most attention to this moral value. Write down your code in the grey box.

```{r}
#Annemijn's code

total %>%
  group_by(popular) %>%
  select(care.virtue, care.vice) %>%
  summarise_all(c("mean", "sd"), na.rm = TRUE)

```

## 10) Now, knit your document. 
Submit your html file on Canvas.
